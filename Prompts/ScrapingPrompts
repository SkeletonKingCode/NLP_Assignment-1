# Gemini:

## 1

~~~prompt
You are are a Data Science and Scraping expert.
I am using firefox on Ubuntu 24.04 Linux. I want to scrap urdu children stories from urdupoint. What are the best tools for this?
~~~

## 2

~~~prompt
This is the html for the stories list page and an example story page write a python script to scrap data of these stories into text along appending tags, <EOS> end of sentence, <EOP> end of paragraph and <EOP> end of story
>>HTML of "Moral Stories Page" and "Garor ki Saza"
~~~

## 3

~~~prompt
Yes, thank you for correcting my EOD mistake
https://www.urdupoint.com/kids/category/moral-stories-page1.html
Link, pages increase -page1 -> -page2 -> -page3-> ... -> -page100
~~~


## 4

~~~prompt
Save the all the links in a file too
~~~


## 5

~~~prompt
The script did not get anything nor give error, I think it was stopped by cloudflare, use undetected selenium webdriver to bypass it.
~~~


## 6

~~~prompt
It is going to links and saving the links but either the text is not getting extracted or it is not saving properly:
```python
import undetected_chromedriver as uc
from bs4 import BeautifulSoup
import re
import time
import random
import os

# Configuration
BASE_URL = "https://www.urdupoint.com"
LIST_URL_TEMPLATE = "https://www.urdupoint.com/kids/category/moral-stories-page{}.html"
OUTPUT_DIR = "urdu_stories_dataset"
LINKS_FILE = "all_story_links.txt"

# Requested Tags
TAG_EOS = " <EOS>" # End of Sentence
TAG_EOP = " <EOP>" # End of Paragraph
TAG_EOD = " <EOD>" # End of Story

# Urdu Sentence Punctuation Regex (Full stop, Question mark, Exclamation)
URDU_PUNCT_REGEX = r'([Û”ØŸ!])'

if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

def process_urdu_text(text):
    """Splits text into sentences and appends <EOS> tags."""
    if not text: return ""
    # Split by punctuation but keep the mark
    parts = re.split(URDU_PUNCT_REGEX, text)
    sentences = []
    for i in range(0, len(parts) - 1, 2):
        sentence = parts[i].strip() + parts[i+1]
        if sentence:
            sentences.append(sentence + TAG_EOS)
    
    # Catch trailing text without punctuation
    if len(parts) % 2 != 0 and parts[-1].strip():
        sentences.append(parts[-1].strip() + TAG_EOS)
        
    return " ".join(sentences)

def get_driver():
    """Initializes the undetected chrome driver."""
    options = uc.ChromeOptions()
    options.add_argument("--no-sandbox") # Required for some Ubuntu environments
    options.add_argument("--disable-dev-shm-usage")
    # Uncomment the next line if you want the window to be invisible:
    # options.add_argument("--headless") 
    
    driver = uc.Chrome(options=options)
    return driver

def scrape_urdu_point(start_page, end_page):
    driver = get_driver()
    
    try:
        with open(LINKS_FILE, "a", encoding="utf-8") as links_log:
            for page_num in range(start_page, end_page + 1):
                print(f"\n--- ğŸ“„ Navigating to List Page {page_num} ---")
                list_url = LIST_URL_TEMPLATE.format(page_num)
                
                driver.get(list_url)
                # Random sleep to mimic human behavior
                time.sleep(random.uniform(3, 6)) 
                
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                # Find links with class 'sharp_box' as identified in your HTML
                story_links = soup.find_all('a', class_='sharp_box')
                
                if not story_links:
                    print("âš ï¸ No links found. Cloudflare might be blocking or selectors changed.")
                    continue

                for link in story_links:
                    href = link.get('href')
                    if not href: continue
                    full_url = href if href.startswith('http') else BASE_URL + href
                    
                    # Log the link
                    links_log.write(full_url + "\n")
                    links_log.flush()

                    # Visit the story page
                    print(f"   ğŸ“– Scraping: {full_url}")
                    driver.get(full_url)
                    time.sleep(random.uniform(0,.5))
                    
                    story_soup = BeautifulSoup(driver.page_source, 'html.parser')
                    
                    # Extract Title
                    title_tag = story_soup.find('h1')
                    title = title_tag.get_text(strip=True) if title_tag else "Untitled"

                    # Content is usually in 'detail_text_ads'
                    content_div = story_soup.find('div', class_='detail_text_ads')
                    if not content_div: continue

                    # Find paragraphs using 'urdu' or 'ar-huruf' classes
                    p_tags = content_div.find_all(['p', 'span'], class_=re.compile(r'urdu|ar-huruf'))
                    
                    processed_paragraphs = []
                    for p in p_tags:
                        txt = p.get_text(strip=True)
                        if len(txt) > 20:
                            processed_txt = process_urdu_text(txt)
                            processed_paragraphs.append(processed_txt + TAG_EOP)

                    if processed_paragraphs:
                        # Save result
                        filename = "".join(x for x in title[:50] if x.isalnum() or x==' ').strip()
                        file_path = os.path.join(OUTPUT_DIR, f"{filename}.txt")
                        
                        with open(file_path, "w", encoding="utf-8") as f:
                            f.write(f"TITLE: {title}\n")
                            f.write("\n".join(processed_paragraphs))
                            f.write(TAG_EOD)
                        
                    # Delay between stories
                    time.sleep(random.uniform(1, 2))

    finally:
        driver.quit()

if __name__ == "__main__":
    scrape_urdu_point(1, 100)
```
do you need the example html structure again?

~~~


## 7

~~~prompt
>>HTML of "Garor ki Saza" again
~~~



## 8

~~~prompt
Titles and article number <h2 class="urdu fs24 lh48 ar rtl txt_blue"><span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">ØºØ±ÙˆØ± Ú©ÛŒ Ø³Ø²Ø§</span> - <span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">ØªØ­Ø±ÛŒØ± Ù†Ù…Ø¨Ø±</span> 2922</h2>

All paragraphs are in <span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;"> urdu-text</span>, title in 
~~~

## 9 New Chat

~~~prompt
You are an data scraping and python expert.
Complete python script to scrap data of these stories into text along appending tags, <EOS> end of sentence use function, <EOP> end of paragraph since each span is a paragraph use that and <EOD> end of story  . Print the title and 1st span so I know if the code it working:

Title Tag:
```html
<h1 class="phead">Gharoor Ki Saza - Article No. 2922</h1>
```

Story Paragraph exact tags and css:
```html
<div id="main_wrap">
Â  Â  <div class="container" id="main_content">
Â  Â  Â  Â  <div class="main_bar">
Â  Â  Â  Â  Â  Â  <div class="shad_box mb10">
Â  Â  Â  Â  Â  Â  Â  Â  <div class="txt_detail urdu ar rtl">
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <div style="text-align: right;">
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ø§Ø­Ø³Ù† Ø³Ø§ØªÙˆÛŒÚº Ø¬Ù…Ø§Ø¹Øª Ú©Ø§ Ø·Ø§Ù„Ø¨ Ø¹Ù„Ù… ØªÚ¾Ø§Û”ÙˆÛ Ø§Ù¾Ù†Û’ Ù…Ø§Úº Ø¨Ø§Ù¾ Ú©Û’ Ø³Ø§ØªÚ¾ Ø¯Ø±ÛŒØ§ Ú©Ù†Ø§Ø±Û’ Ú†Ú¾ÙˆÙ¹Û’ Ø³Û’ Ú¯Ú¾Ø± Ù…ÛŒÚº Ø±ÛØªØ§ ØªÚ¾Ø§Û”ÙˆÛ Ù„ÙˆÚ¯ Ø¨ÛØª ØºØ±ÛŒØ¨ ØªÚ¾Û’Û”Ø§Ø­Ø³Ù† Ú©Û’ Ø§Ø¨Ùˆ Ø§ÛŒÚ© Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ù…Ú†Ú¾ÛŒØ±Û’ ØªÚ¾Û’Û”ÙˆÛ Ø¯Ù† Ø¨Ú¾Ø± Ù…Ú†Ú¾Ù„ÛŒØ§Úº Ù¾Ú©Ú‘ØªÛ’ØŒ Ù…Ú¯Ø± Ø§Ù† Ú©Û’ ÛØ§ØªÚ¾ Ø¨ÛØª Ú©Ù… Ù…Ú†Ú¾Ù„ÛŒØ§Úº Ø¢ØªÛŒ ØªÚ¾ÛŒÚºÛ”Ú©Ø¨Ú¾ÛŒ ØªÙˆ Ø§Ù† Ú©Û’ Ú¯Ú¾Ø± Ù…ÛŒÚº ÙØ§Ù‚Û’ Ø¨Ú¾ÛŒ ÛÙˆ Ø¬Ø§ØªÛ’ ØªÚ¾Û’Û”</span>
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  </div>
Â  Â  Â  Â  Â  Â  Â  Â  </div>
Â  Â  Â  Â  Â  Â  </div>
Â  Â  Â  Â  </div>
Â  Â  </div>
</div>
```

```python
import undetected_chromedriver as uc
from bs4 import BeautifulSoup
import re
import time
import random
import os

# Configuration
BASE_URL = "https://www.urdupoint.com"
LIST_URL_TEMPLATE = "https://www.urdupoint.com/kids/category/moral-stories-page{}.html"
OUTPUT_DIR = "urdu_stories_dataset"
LINKS_FILE = "all_story_links.txt"

# Requested Tags
TAG_EOS = " <EOS>"Â 
TAG_EOP = " <EOP>"Â 
TAG_EOD = " <EOD>"Â 

# Urdu Sentence Punctuation Regex
URDU_PUNCT_REGEX = r'([Û”ØŸ!])'

if not os.path.exists(OUTPUT_DIR):
Â  Â  os.makedirs(OUTPUT_DIR)

def process_urdu_text(text):
Â  Â  if not text: return ""
Â  Â  parts = re.split(URDU_PUNCT_REGEX, text)
Â  Â  sentences = []
Â  Â  for i in range(0, len(parts) - 1, 2):
Â  Â  Â  Â  sentence = parts[i].strip() + parts[i+1]
Â  Â  Â  Â  if sentence:
Â  Â  Â  Â  Â  Â  sentences.append(sentence + TAG_EOS)
Â  Â  if len(parts) % 2 != 0 and parts[-1].strip():
Â  Â  Â  Â  sentences.append(parts[-1].strip() + TAG_EOS)
Â  Â  return " ".join(sentences)

def get_driver():
Â  Â  options = uc.ChromeOptions()
Â  Â  options.add_argument("--no-sandbox")Â 
Â  Â  options.add_argument("--disable-dev-shm-usage")
Â  Â  # Using windowed mode helps bypass initial Cloudflare checks
Â  Â  driver = uc.Chrome(options=options)
Â  Â  return driver

def scrape_urdu_point(start_page, end_page):
Â  Â  driver = get_driver()
Â  Â  try:
Â  Â  Â  Â  with open(LINKS_FILE, "a", encoding="utf-8") as links_log:
Â  Â  Â  Â  Â  Â  for page_num in range(start_page, end_page + 1):
Â  Â  Â  Â  Â  Â  Â  Â  print(f"\n--- Story List Page {page_num} ---")
Â  Â  Â  Â  Â  Â  Â  Â  driver.get(LIST_URL_TEMPLATE.format(page_num))
Â  Â  Â  Â  Â  Â  Â  Â  time.sleep(random.uniform(4, 6))Â 
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  soup = BeautifulSoup(driver.page_source, 'html.parser')
Â  Â  Â  Â  Â  Â  Â  Â  story_links = soup.find_all('a', class_='sharp_box')
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  for link in story_links:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  href = link.get('href')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not href: continue
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  full_url = href if href.startswith('http') else BASE_URL + href
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  links_log.write(full_url + "\n")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  links_log.flush()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"Â  Â Scraping: {full_url}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  driver.get(full_url)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Wait for Urdu font scripts to render
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  time.sleep(random.uniform(3, 5))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  story_soup = BeautifulSoup(driver.page_source, 'html.parser')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ### Write Text extraction Code

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if processed_paragraphs:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  safe_title = "".join(x for x in title[:50] if x.isalnum() or x==' ').strip()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_path = os.path.join(OUTPUT_DIR, f"{safe_title}.txt")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with open(file_path, "w", encoding="utf-8") as f:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f.write(f"TITLE: {title}\n\n")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f.write("\n".join(processed_paragraphs))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f.write(TAG_EOD)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  print(f"Â  Â  Â  âœ… Saved: {safe_title}.txt")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  time.sleep(random.uniform(1, 2))
Â  Â  finally:
Â  Â  Â  Â  driver.quit()

if __name__ == "__main__":
Â  Â  scrape_urdu_point(1, 100)
```
~~~

# DeepSeek:

## 1

~~~prompt
import undetected_chromedriver as uc
from bs4 import BeautifulSoup
import re
import time
import random
import os

# Configuration
BASE_URL = "https://www.urdupoint.com"
LIST_URL_TEMPLATE = "https://www.urdupoint.com/kids/category/moral-stories-page{}.html"
OUTPUT_DIR = "urdu_stories_dataset"
LINKS_FILE = "all_story_links.txt"

# Requested Tags
TAG_EOS = " <EOS>" 
TAG_EOP = " <EOP>" 
TAG_EOD = " <EOD>" 

# Urdu Sentence Punctuation Regex - Handles typical Urdu stops
URDU_PUNCT_REGEX = r'([Û”ØŸ!])'

if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

def process_urdu_text(text):
    if not text: return ""
    # Split text by punctuation while keeping the punctuation mark
    parts = re.split(URDU_PUNCT_REGEX, text)
    sentences = []
    
    # Reassemble sentences with the punctuation and add EOS tag
    for i in range(0, len(parts) - 1, 2):
        sentence = parts[i].strip() + parts[i+1]
        if sentence:
            sentences.append(sentence + TAG_EOS)
            
    # Handle any trailing text without punctuation
    if len(parts) % 2 != 0 and parts[-1].strip():
        sentences.append(parts[-1].strip() + TAG_EOS)
        
    return " ".join(sentences)

def get_driver():
    options = uc.ChromeOptions()
    options.add_argument("--no-sandbox") 
    options.add_argument("--disable-dev-shm-usage")
    options.page_load_strategy("eager")
    driver = uc.Chrome(options=options)
    return driver

def scrape_urdu_point(start_page, end_page):
    driver = get_driver()
    try:
        with open(LINKS_FILE, "a", encoding="utf-8") as links_log:
            for page_num in range(start_page, end_page + 1):
                print(f"\n--- Story List Page {page_num} ---")
                driver.get(LIST_URL_TEMPLATE.format(page_num))
                time.sleep(random.uniform(1, 2)) 
                
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                # Selecting the specific story link boxes
                story_links = soup.find_all('a', class_='sharp_box')
                
                for link in story_links:
                    href = link.get('href')
                    if not href: continue
                    full_url = href if href.startswith('http') else BASE_URL + href
                    
                    links_log.write(full_url + "\n")
                    links_log.flush()

                    print(f"   Scraping: {full_url}")
                    driver.get(full_url)
                    time.sleep(random.uniform(3, 5))
                    
                    story_soup = BeautifulSoup(driver.page_source, 'html.parser')
                    
                    # --- Text Extraction Logic ---
                    # 1. Get the Title
                    title_tag = story_soup.find('h1', class_='phead')
                    title = title_tag.get_text(strip=True) if title_tag else "Untitled Story"
                    print(title)

                    # 2. Get the Paragraphs (Spans inside the specific div)
                    processed_paragraphs = []
                    # Find all spans within div[style="text-align: right;"] that have the class 'nastaleeq3'
                    # Option 1: If the structure is exactly as shown
                
                    spans = story_soup.find_all("span", style="font-size:1.25em; line-height:1.8em;")
                    print(spans)
                    for i, span in enumerate(spans):
                        raw_text = span.get_text(strip=True)
                        print(raw_text)
                        if raw_text:
                            # Process sentences and add <EOP>
                            p_text = process_urdu_text(raw_text) + TAG_EOP
                            processed_paragraphs.append(p_text)
                            
                            # Print 1st span of 1st paragraph for debugging as requested
                            if i == 0:
                                print(f"   [DEBUG] Title: {title}")
                                print(f"   [DEBUG] 1st Span: {p_text[:100]}...")

                    # 3. Save to file
                    if processed_paragraphs:
                        # Clean title for filename
                        safe_title = "".join(x for x in title[:50] if x.isalnum() or x==' ').strip()
                        file_path = os.path.join(OUTPUT_DIR, f"{safe_title}.txt")
                        
                        with open(file_path, "w", encoding="utf-8") as f:
                            # Note: We don't add tags to the Title unless you want them there too
                            f.write(f"TITLE: {title}\n\n")
                            f.write("\n".join(processed_paragraphs))
                            f.write(TAG_EOD)
                        print(f"      Saved: {safe_title}.txt")

                    time.sleep(random.uniform(1, 2))
    finally:
        driver.quit()

if __name__ == "__main__":
    scrape_urdu_point(1, 100)

Change the code so it use selenium instead of beautiful soup for parsing
~~~

---

## 2

~~~prompt
--- Story List Page 1 ---
   Scraping: https://www.urdupoint.com/kids/detail/moral-stories/allah-ka-dost-2923.html
Allah Ka Dost - Article No. 2923
Found 0 spans
Traceback (most recent call last):
  File "/home/syedmalyun/Code/Semester-6/NLP/Theory/Assignments/Assignment-1/Scraping/scarper.py", line 125, in <module>
    scrape_urdu_point(1, 100)
  File "/home/syedmalyun/Code/Semester-6/NLP/Theory/Assignments/Assignment-1/Scraping/scarper.py", line 66, in scrape_urdu_point
    href = link.get_attribute('href')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/syedmalyun/Applications/miniconda/envs/urdu-scraper/lib/python3.11/site-packages/selenium/webdriver/remote/webelement.py", line 213, in get_attribute
    attribute_value = self.parent.execute_script(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/syedmalyun/Applications/miniconda/envs/urdu-scraper/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 518, in execute_script
    return self.execute(command, {"script": script, "args": converted_args})["value"]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/syedmalyun/Applications/miniconda/envs/urdu-scraper/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py", line 432, in execute
    self.error_handler.check_response(response)
  File "/home/syedmalyun/Applications/miniconda/envs/urdu-scraper/lib/python3.11/site-packages/selenium/webdriver/remote/errorhandler.py", line 232, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.StaleElementReferenceException: Message: stale element reference: stale element not found
  (Session info: chrome=145.0.7632.75); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#staleelementreferenceexception
Stacktrace:
#0 0x5dd891486cca <unknown>
#1 0x5dd890e96682 <unknown>
#2 0x5dd890eab09b <unknown>
#3 0x5dd890ea9b31 <unknown>
#4 0x5dd890e9e1b9 <unknown>
#5 0x5dd890e9e2f3 <unknown>
#6 0x5dd890e9c369 <unknown>
#7 0x5dd890ea0339 <unknown>
#8 0x5dd890f350d9 <unknown>
#9 0x5dd890f3404b <unknown>
#10 0x5dd890edd88f <unknown>
#11 0x5dd890ede651 <unknown>
#12 0x5dd89144b119 <unknown>
#13 0x5dd89144e021 <unknown>
#14 0x5dd8914378d9 <unknown>
#15 0x5dd89144ebee <unknown>
#16 0x5dd89141dc50 <unknown>
#17 0x5dd891473318 <unknown>
#18 0x5dd8914734e8 <unknown>
#19 0x5dd891485313 <unknown>
#20 0x77a5ada9caa4 <unknown>
#21 0x77a5adb29c6c <unknown>
~~~

---

## 3

~~~prompt
It is only getting the Author name, instead of using span get the whole text from the div and separate paragraphs perhaps by '\n' or  another method. Make sure it correctly inserts the special tokens for end of sentence, paragraph and story
```output
...
--- Story 1/12 ---
   Scraping: https://www.urdupoint.com/kids/detail/moral-stories/allah-ka-dost-2923.html
   Title: Allah Ka Dost - Article No. 2923
   Found 3 elements with selector: div[style='text-align: right;'] span
[<undetected_chromedriver.webelement.WebElement (session="440805419d6a73865b151b5be6895805", element="f.C23D8A7DBAE58338540F5ED2286C22D0.d.7C861B1C70569B22724CA30A5D891B78.e.221")>, <undetected_chromedriver.webelement.WebElement (session="440805419d6a73865b151b5be6895805", element="f.C23D8A7DBAE58338540F5ED2286C22D0.d.7C861B1C70569B22724CA30A5D891B78.e.222")>, <undetected_chromedriver.webelement.WebElement (session="440805419d6a73865b151b5be6895805", element="f.C23D8A7DBAE58338540F5ED2286C22D0.d.7C861B1C70569B22724CA30A5D891B78.e.223")>]
   [DEBUG] 1st Paragraph Preview: Ù…Ø­Ù…Ø¯ Ø´Ø§ÛØ¯ Ø­ÙÛŒØ¸ <EOS> <EOP>...
      Saved: Allah Ka Dost  Article No 2923.txt
...
```
<div id="main_wrap">
    <div class="container" id="main_content">
        <div class="main_bar">
            <div class="shad_box mb10">
                <div class="txt_detail urdu ar rtl">
                    <div style="text-align: right;"><span style="color: rgb(255, 0, 0); --darkreader-inline-color: var(--darkreader-text-ff0000, #ff2727);" data-darkreader-inline-color=""><span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ù…Ø­Ù…Ø¯ Ø´Ø§ÛØ¯ Ø­ÙÛŒØ¸</span></span><br><span style="color: rgb(255, 0, 0); --darkreader-inline-color: var(--darkreader-text-ff0000, #ff2727);" data-darkreader-inline-color=""></span><span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ù…ÛŒÚº Ø§ÛŒÚ© Ø§Ø³ØªØ§Ø¯ ÛÙˆÚº Ø§ÙˆØ± Ù…ÛŒØ±Ø§ Ù…Ø¶Ù…ÙˆÙ† Ø§Ø³Ù„Ø§Ù…ÛŒØ§Øª ÛÛ’Û”Ù†Ø¦Û’ Ø§Ø³Ú©ÙˆÙ„ Ù…ÛŒÚº Ø¢Ø¬ Ù…ÛŒØ±Ø§ Ù¾ÛÙ„Ø§ Ø¯Ù† ØªÚ¾Ø§Û”Ø§Ø³ÛŒ ÙˆØ¬Û Ø³Û’ Ø®ÙˆØ´ÛŒ Ø¨Ú¾ÛŒ ØªÚ¾ÛŒ Ø§ÙˆØ± ÚˆØ± Ø¨Ú¾ÛŒÛ”Ø§Ø³ÛŒ Ø®ÙˆØ´ÛŒ Ù…ÛŒÚºØŒ Ù…ÛŒÚº Ù†Û’ Ù†Ø§Ø´ØªÛ Ø¨Ú¾ÛŒ Ø¨Ø±Ø§Ø¦Û’ Ù†Ø§Ù… Ú©ÛŒØ§ Ø§ÙˆØ± ÙˆÙ‚Øª Ø³Û’ Ú©Ú†Ú¾ Ø¯ÛŒØ± Ù¾ÛÙ„Û’ ÛÛŒ Ø§Ø³Ú©ÙˆÙ„ Ù¾ÛÙ†Ú† Ú¯ÛŒØ§Û”Ù¾Ø±Ù†Ø³Ù¾Ù„ ØµØ§Ø­Ø¨ Ø³Û’ Ù…Ù„Ù†Û’ Ú©Û’ Ø¨Ø¹Ø¯ Ù…Ø¬Ú¾Û’ Ø§ÛŒÚ© Ú©Ù„Ø§Ø³ Ù…ÛŒÚº Ø¨Ú¾ÛŒØ¬ Ø¯ÛŒØ§ Ú¯ÛŒØ§Û”</span><div class="clear mt2"></div><span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ú©Ù„Ø§Ø³ Ø±ÙˆÙ… Ù…ÛŒÚº Ø®ÙˆØ¨ Ø´ÙˆØ± ÛÙˆ Ø±ÛØ§ ØªÚ¾Ø§Û”ØªÙ…Ø§Ù… Ø¨Ú†Û’ Ø§Ù¾Ù†ÛŒ Ø¹Ø§Ø¯Øª Ùˆ ÙØ·Ø±Øª Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ Ø²ÙˆØ± Ø²ÙˆØ± Ø³Û’ Ø¨Ø§ØªÛŒÚº Ú©Ø± Ø±ÛÛ’ ØªÚ¾Û’Û”Ù…ÛŒÚº Ù†Û’ Ú©Ù…Ø±Û’ Ù…ÛŒÚº Ù‚Ø¯Ù… Ø±Ú©Ú¾Ø§ ØªÙˆ Ø³Ø¨ Ú©Ùˆ Ø³Ø§Ù†Ù¾ Ø³ÙˆÙ†Ú¯Ú¾ Ú¯ÛŒØ§Û”Ø³Ø¨ Ú©Û’ Ø³Ø¨ Ø®Ø§Ù…ÙˆØ´ÛŒ Ø³Û’ Ø³ÛŒØ¯Ú¾Û’ Ø¨ÛŒÙ¹Ú¾ Ú¯Ø¦Û’Û”Ù¾Ú¾Ø± Ø§Ú†Ø§Ù†Ú© Ú©Ù„Ø§Ø³ Ú©ÛŒ Ø¯Ø§Ø¦ÛŒÚº Ø¬Ø§Ù†Ø¨ Ø³Û’</span> â€<span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ú©Ù„Ø§Ø³ Ø§Ø³Ù¹ÛŒÙ†Úˆ</span>â€œ <span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ú©ÛŒ Ø¢ÙˆØ§Ø² Ú¯ÙˆÙ†Ø¬ÛŒÛ”ÛŒÛ ÛŒÙ‚ÛŒÙ†Ø§ Ú©Ù„Ø§Ø³ Ù…Ø§Ù†ÛŒÙ¹Ø± ØªÚ¾Ø§Û”Ø§Ø³ Ú©ÛŒ Ø¢ÙˆØ§Ø² Ø³Ù† Ú©Ø± Ø³Ø¨ Ø¨Ú†Û’ Ø¨Ø§ Ø§Ø¯Ø¨ Ú©Ú¾Ú‘Û’ ÛÙˆ Ú¯Ø¦Û’Û”</span><br>â€<span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ø³ÙÙ¹ ÚˆØ§Ø¤Ù† Ù¾Ù„ÛŒØ²Û”</span><p></p><p class="hide_desk ac urdu rtl fs18 lh36" style="color: rgb(102, 102, 102); --darkreader-inline-color: var(--darkreader-text-666666, #fcf0e0);" data-darkreader-inline-color="">(<span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ø¬Ø§Ø±ÛŒ ÛÛ’</span>)</p><div id="teads_upw"><div id="teads"></div></div>


<div class="clear"></div>
<div align="center" style="text-align:center; padding-top:0px;padding-bottom:0px; margin:0 auto;">
	<div id="div-gpt-ad-outstream-wrap"><div id="div-gpt-ad-outstream"></div></div>
    <div id="div-gpt-ad-1x1-wrap"><div id="div-gpt-ad-1x1"></div></div>
</div>
<div class="clear"></div>

<script>
	document.getElementById("teads_upw").innerHTML = '<div id="teads"></div>';
	
	document.getElementById("div-gpt-ad-outstream-wrap").innerHTML = '<div id="div-gpt-ad-outstream"></div>';
	document.getElementById("div-gpt-ad-1x1-wrap").innerHTML = '<div id="div-gpt-ad-1x1"></div>';
	
	googletag.cmd.push(function() { googletag.display("div-gpt-ad-outstream"); });
	googletag.cmd.push(function() { googletag.display("div-gpt-ad-1x1"); });
</script><div id="eng_mob_ad_wrap"><script>
	if(upgj_x <= 800){
		//googletag.defineSlot('/21678054/up-v2/mobile-middle', [300, 250], 'gpt-middle-banner').addService(googletag.pubads());
		document.getElementById("div-gpt-ad-outstream-wrap").innerHTML = '<div id="gpt-middle-banner-in"><div id="gpt-middle-banner"></div></div>';
		googletag.cmd.push(function() { googletag.display('gpt-middle-banner'); });
	}
</script></div><p style="margin-top:0px;">â€œ <span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ù…ÛŒÚº Ù†Û’ ÛØ§ØªÚ¾ Ø³Û’ Ø§Ø´Ø§Ø±Û Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ú©ÛØ§Û”</span></p><div class="clear mt2"></div><span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ø³Ø¨ Ø¨ÛŒÙ¹Ú¾ Ú¯Ø¦Û’Û”</span><br>â€<span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ø´Ø§ÛŒØ¯ Ø¢Ù¾ Ø¬Ø§Ù†ØªÛ’ ÛÙˆÚº Ú©Û Ø§Ø¨ØŒ Ù…ÛŒÚº Ø¢Ù¾ Ú©Ùˆ Ø§Ø³Ù„Ø§Ù…ÛŒØ§Øª Ù¾Ú‘Ú¾Ø§ÛŒØ§ Ú©Ø±ÙˆÚº Ú¯Ø§Û”</span>â€œ<br>â€<span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">ÛŒØ³ Ø³Ø±</span>!â€œ <span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ú†Ù†Ø¯ Ø¨Ú†ÙˆÚº Ú©ÛŒ Ø¢ÙˆØ§Ø² Ø¢Ø¦ÛŒÛ”</span><br>â€<span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ø¢Ø¬ Ù¾ÛÙ„ÛŒ Ø¨Ø§Ø± Ø¢Ù¾ Ú©ÛŒ Ú©Ù„Ø§Ø³ Ù„Û’ Ø±ÛØ§ ÛÙˆÚºØŒ Ø§Ø³ÛŒ Ù„Ø¦Û’ Ø¢Ø¬ Ú©Ú†Ú¾ Ù†ÛÛŒÚº Ù¾Ú‘Ú¾Ø§Ø¤Úº Ú¯Ø§Û”Ù¾ÛÙ„Û’ Ù…ÛŒÚº Ø§Ù¾Ù†Ø§ ØªØ¹Ø§Ø±Ù Ú©Ø±Ø§Ø¤Úº Ú¯Ø§ØŒ Ù¾Ú¾Ø± Ø§ÛŒÚ© Ø§ÛŒÚ© Ú©Ø± Ú©Û’ Ø¢Ù¾ Ø³Ø¨ Ú©Û’ Ø¨Ø§Ø±Û’ Ù…ÛŒÚº Ø¬Ø§Ù†Ù†Ø§ Ú†Ø§ÛÙˆÚº Ú¯Ø§Û”</span>â€œ<br><span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ø§Ø³ØªØ§Ø¯ Ú©Ø§ Ø´Ø§Ú¯Ø±Ø¯ÙˆÚº Ø³Û’ Ø¨Ú‘Ø§ Ú¯ÛØ±Ø§ ØªØ¹Ù„Ù‚ ÛÙˆØªØ§ ÛÛ’Û”Ø§Ø³ØªØ§Ø¯ Ù…Ø¹Ù„Ù… ÛÛ’ Ø§ÙˆØ± Ø§Ø³ Ú©Ø§ Ú©Ø§Ù… Ø¹Ù„Ù… Ùˆ Ø¢Ú¯ÛÛŒ Ø¯ÛŒÙ†Ø§ ÛÛ’Û”</span><div class="clear mt2"></div><span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">ÛŒÛÛŒ Ú©Ø§Ù… Ú¯Ú¾Ø± Ù…ÛŒÚº Ù…Ø§Úº Ø¨Ø§Ù¾ Ø¨Ú¾ÛŒ Ú©Ø±ØªÛ’ ÛÛŒÚºÛ”Ø§Ø³ÛŒ Ù„Ø­Ø§Ø¸ Ø³Û’ Ú©ÛØ§ Ø¬Ø§ØªØ§ ÛÛ’ Ú©Û Ø§Ø³ØªØ§Ø¯ Ø¨Ø§Ù¾ Ú©ÛŒ Ø¬Ú¯Û ÛÙˆØªØ§ ÛÛ’ØŒ Ù„ÛŒÚ©Ù† Ù…ÛŒØ±Û’ Ù†Ø²Ø¯ÛŒÚ© Ø§Ø³ØªØ§Ø¯ ÛÛŒ Ø¨ÛØªØ±ÛŒÙ† Ø¯ÙˆØ³Øª ÛÛ’Û”Ø´Ø§Ú¯Ø±Ø¯ÙˆÚº Ú©Ùˆ Ø§Ø³ØªØ§Ø¯ Ú©Ø§ Ø§Ø­ØªØ±Ø§Ù… Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ø§Ø³ Ø³Û’ Ø¨Û’ ØªÚ©Ù„Ù Ø¨Ú¾ÛŒ ÛÙˆÙ†Ø§ Ú†Ø§ÛÛŒÛ’ØŒ ØªØ§Ú©Û ÙˆÛ Ø§Ù¾Ù†Û’ Ù…Ø³Ø§Ø¦Ù„ Ù¾Ø± Ø§Ø³ØªØ§Ø¯ Ø³Û’ Ø¨Ø§Øª Ú©Ø± Ø³Ú©ÛŒÚºÛ”Ø§Ø³ Ø³Û’ Ù…Ø´ÙˆØ±Û Ú©Ø± Ø³Ú©ÛŒÚº Ø§ÙˆØ± Ø§Ø³ØªØ§Ø¯ Ø§Ù† Ú©ÛŒ Ø±Ø§ÛÙ†Ù…Ø§Ø¦ÛŒ Ú©Ø± Ø³Ú©Û’Û”Ú†Ù†Ø¯ Ù„Ù…Ø­Û’ Ú©Ù„Ø§Ø³ Ù…ÛŒÚº Ø®Ø§Ù…ÙˆØ´ÛŒ Ú†Ú¾Ø§Ø¦ÛŒ Ø±ÛÛŒ Ù¾Ú¾Ø± Ú©Ú†Ú¾ Ù…Ù„ÛŒ Ø¬Ù„ÛŒ Ø¢ÙˆØ§Ø²ÛŒÚº Ø§ÙØ¨Ú¾Ø±ÛŒÚºÛ”</span><div class="clear mt2"></div><br>â€<span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">ØªÚ¾ÛŒÙ†Ú© ÛŒÙˆ Ø³Ø±</span>!â€œ<br>â€<span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ø§Ø¨ Ù…ÛŒÚº Ø§Ù¾Ù†Ø§ ØªØ¹Ø§Ø±Ù Ú©Ø±Ø§ Ø¯ÙˆÚºÛ”Ù…ÛŒØ±Ø§ Ù†Ø§Ù… Ø´ÛØ§Ø¨ Ø­ÛŒØ¯Ø± ÛÛ’Û”ØµØ­Ø§ÙØª Ù…ÛŒÚº Ø§ÛŒÙ… Ø§Û’ Ú©ÛŒØ§ ÛÛ’ØŒ Ù…Ú¯Ø± Ø¹Ù…Ù„ÛŒ Ø·ÙˆØ± Ù¾Ø± ØªØ¯Ø±ÛŒØ³ÛŒ Ù…ÛŒØ¯Ø§Ù† Ù…ÛŒÚº Ø¢ Ú¯ÛŒØ§Û”Ù¾Ú‘Ú¾Ø§Ù†Ø§ Ù…ÛŒØ±Ø§ Ø´ÙˆÙ‚ ÛÛŒ Ù†ÛÛŒÚºØŒ Ø¨Ù„Ú©Û Ø§Ø³ Ø³Û’ Ù…Ø¬Ú¾Û’ Ø¯Ù„ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† ÛÙˆØªØ§ ÛÛ’Û”Ù…ÛŒÚº Ù†Û’ Ø§Ø³Ù„Ø§Ù…ÛŒØ§Øª Ú©Ø§ Ù…Ø¶Ù…ÙˆÙ† Ø§Ø³ Ù„Ø¦Û’ Ù…Ù†ØªØ®Ø¨ Ú©ÛŒØ§ ÛÛ’ Ú©Û Ø¢Ù¾ Ú©Ùˆ Ø­Ù‚ÛŒÙ‚ÛŒ Ø§Ø³Ù„Ø§Ù… Ø³Û’ Ø¢Ú¯Ø§Û Ú©Ø±ÙˆÚº Ø§ÙˆØ± Ø¢Ù¾ Ú©Ùˆ Ù…Ø­Ø¨Ù ÙˆØ·Ù† Ø´ÛØ±ÛŒ Ø¨Ù†Ø§Ø¤ÚºÛ”Ø§Ø¨ Ø¢Ù¾ Ù„ÙˆÚ¯ Ø¨Ø§Ø±ÛŒ Ø¨Ø§Ø±ÛŒ Ø§Ù¾Ù†Ø§ ØªØ¹Ø§Ø±Ù Ú©Ø±Ø§ Ø¯ÛŒÚºÛ”</span><div class="clear mt2"></div>â€œ <span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ù…ÛŒÚº Ù†Û’ Ù¾ÛÙ„ÛŒ Ù‚Ø·Ø§Ø± Ù…ÛŒÚº Ø¯Ø§ÛÙ†ÛŒ Ø¬Ø§Ù†Ø¨ Ø¨ÛŒÙ¹Ú¾Û’ Ù„Ú‘Ú©Û’ Ú©ÛŒ Ø·Ø±Ù Ø§Ø´Ø§Ø±Û Ú©ÛŒØ§Û”</span><br>â€<span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ø³Ø±</span>! <span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ù…ÛŒØ±Ø§ Ù†Ø§Ù… Ø¹Ù…Ø± Ø§Ø­Ù…Ø¯ ÛÛ’Û”Ù…ÛŒØ±Û’ Ø§Ø¨Ùˆ Ú©Ø§ Ù†Ø§Ù… Ø­ÛŒØ§Øª Ø§Ø­Ù…Ø¯ ÛÛ’Û”ÙˆÛ Ø§ÛŒÚ© Ø¨ÛŒÙ†Ú© Ù…Ù†ÛŒØ¬Ø± ÛÛŒÚºÛ”</span>â€œ <span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ø§Ø³ Ú©Û’ ÙˆØ§Ù„Ø¯ Ú©Ø§ Ø³Ù† Ú©Ø± Ø³Ø§Ø±ÛŒ Ú©Ù„Ø§Ø³ Ù¾Ø± Ø±Ø¹Ø¨ Ø·Ø§Ø±ÛŒ ÛÙˆ Ú¯ÛŒØ§Û”Ø§ØªÙ†Û’ Ù…ÛŒÚº Ø¯ÙˆØ³Ø±Ø§ Ù„Ú‘Ú©Ø§ Ú©Ú¾Ú‘Ø§ ÛÙˆ Ú¯ÛŒØ§Û”</span><br>â€<span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ø³Ø±</span>! <span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ù…ÛŒØ±Ø§ Ù†Ø§Ù… Ø­Ù…Ø²Û ÛÛ’Û”Ù…ÛŒØ±Û’ Ø§Ø¨Ùˆ Ø§ÛŒÚ© Ù¾Ø±Ø§Ø¦ÛŒÙˆÛŒÙ¹ ÙØ±Ù… Ù…ÛŒÚº Ø¬Ù†Ø±Ù„ Ù…Ù†ÛŒØ¬Ø± ÛÛŒÚºÛ”</span>â€œ<br>â€<span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ø§ÙˆÛ</span>.... <span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">ÛŒÛ Ø¨Ú¾ÛŒ Ù…Ù†ÛŒØ¬Ø±</span>â€¤â€¤â€¤â€¤â€œ <span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ù…ÛŒØ±Û’ Ù…Ù†Û Ø³Û’ Ù†Ú©Ù„Ø§Û”</span><div class="clear mt2"></div><span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ø§Ø³ Ú©Û’ Ø¨Ø¹Ø¯ ØªÛŒØ³Ø±Ø§ Ù„Ú‘Ú©Ø§ Ú©Ú¾Ú‘Ø§ ÛÙˆØ§Û”ÙˆÛ Ø¨Ú¾ÛŒ Ù¾ÛÙ„Û’ Ø¯ÙˆÙ†ÙˆÚº Ø³Û’ Ú©Ù… Ù†Ø¸Ø± Ù†ÛÛŒÚº Ø¢ Ø±ÛØ§ ØªÚ¾Ø§Û”</span><br>â€<span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ù…Ø¬Ú¾Û’ Ø°ÛŒØ´Ø§Ù† Ú©ÛØªÛ’ ÛÛŒÚºÛ”Ù…ÛŒØ±Û’ Ø§Ø¨Ùˆ Ø§ÛŒÚ© ØªØ§Ø¬Ø± ÛÛŒÚº Ø§Ù† Ú©Ø§ Ú©Ù¾Ú‘Û’ Ú©Ø§ Ú©Ø§Ø±ÙˆØ¨Ø§Ø± ÛÛ’Û”</span>â€œ<br><span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ø§Ù† Ø³Ø¨ Ú©Û’ ØªØ¹Ø§Ø±Ù Ù…ÛŒÚº Ø­ÛŒØ±Ø§Ù† Ú©Ù† Ø¨Ø§Øª Ø§Ù† Ú©Ø§ Ø®Ø§Ù†Ø¯Ø§Ù†ÛŒ Ù¾Ø³ Ù…Ù†Ø¸Ø± ØªÚ¾Ø§ØŒ Ø¬Ø³Û’ ÙˆÛ ÙØ®Ø±ÛŒÛ Ø§Ù†Ø¯Ø§Ø² Ù…ÛŒÚº Ø¨ÛŒØ§Ù† Ú©Ø± Ø±ÛÛ’ ØªÚ¾Û’Û”ÛŒÛ Ø¨Ø§Øª Ù…Ø¬Ú¾Û’ Ø§Ú†Ú¾ÛŒ Ù†Û Ù„Ú¯ÛŒÛ”Ø¢Ú¯Û’ Ø¨Ú¾ÛŒ ØªØ¹Ø§Ø±Ù ÛÙˆØ§ ØªÙˆ ØªÙ…Ø§Ù… Ù„Ú‘Ú©Û’ Ø§Ø¹Ù„ÛŒÙ° Ø§ÙˆØ± Ú©Ú¾Ø§ØªÛ’ Ù¾ÛŒØªÛ’ Ú¯Ú¾Ø±Ø§Ù†ÙˆÚº Ú©Û’ Ú†Ø´Ù… Ùˆ Ú†Ø±Ø§Øº Ø«Ø§Ø¨Øª ÛÙˆØ¦Û’ØŒ Ú©ÛŒÙˆÙ†Ú©Û ÛŒÛ Ø§ÛŒÚ© Ù…ÛÙ†Ú¯Ø§ Ø§ÙˆØ± Ù…Ø¹ÛŒØ§Ø±ÛŒ Ø§Ø³Ú©ÙˆÙ„ ØªÚ¾Ø§Û”</span><div class="clear mt2"></div><span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">ØºØ±ÛŒØ¨ Ù„ÙˆÚ¯ÙˆÚº Ú©Û’ Ø¨Ú†Û’ ØªÙˆ Ø§Ø³ Ú©Ø§ ØµØ±Ù Ø³ÙˆÚ† Ø³Ú©ØªÛ’ ØªÚ¾Û’Û”Ø§Ø¨Ú¾ÛŒ Ø§Ù†Ú¾ÛŒ Ø®ÛŒØ§Ù„ÙˆÚº Ù…ÛŒÚº Ù…Ú¯Ù† ØªÚ¾Ø§ Ú©Û Ø§ÛŒÚ© Ù„Ú‘Ú©Ø§ Ø¬Ùˆ Ù„Ø§Ø¦Ù† Ú©Û’ Ø¢Ø®Ø±ÛŒ ÚˆÛŒØ³Ú© Ù¾Ø± Ø¨ÛŒÙ¹Ú¾Ø§ ØªÚ¾Ø§ØŒ Ø§ÙÙ¹Ú¾ Ú©Ú¾Ú‘Ø§ ÛÙˆØ§Û”Ø§Ø³ Ú©Ø§ Ú†ÛØ±Û Ø§Ø¹ØªÙ…Ø§Ø¯ Ø³Û’ Ø®Ø§Ù„ÛŒ Ù†Ø¸Ø± Ø¢ Ø±ÛØ§ ØªÚ¾Ø§Û”Ù…ÛŒÚº Ù†Û’ Ø§Ø³ Ø³Û’ ØªØ¹Ø§Ø±Ù Ú©Û’ Ù„Ø¦Û’ Ú©ÛØ§ ØªÙˆ ÙˆÛ Ù‚Ø¯Ø±Û’ ÛÚ†Ú©Ú†Ø§ØªÛ’ ÛÙˆØ¦Û’ Ø¨ÙˆÙ„Ø§</span>:â€<span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ø³Ø±</span>! <span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ù…ÛŒØ±Ø§ Ù†Ø§Ù… Ù…Ø­Ù…Ø¯ Ø¹Ù„ÛŒ ÛÛ’Û”Ù…ÛŒØ±Ø§ ØªØ¹Ù„Ù‚ Ø§ÛŒÚ© Ø¹Ø§Ù… Ø³Û’ Ú¯Ú¾Ø±Ø§Ù†Û’ Ø³Û’ ÛÛ’Û”</span>â€œ <span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ú©Ù„Ø§Ø³ Ú©Û’ ØªÙ…Ø§Ù… Ù„Ú‘Ú©Û’ Ø§Ø³ Ú©ÛŒ Ø·Ø±Ù Ø¯ÛŒÚ©Ú¾Ù†Û’ Ù„Ú¯Û’ ØªÙˆ ÙˆÛ Ø´Ø±Ù…Ù†Ø¯Û Ø³Ø§ ÛÙˆ Ú¯ÛŒØ§Û”</span><div class="clear mt2"></div><br>â€<span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ø¢Ù¾ Ú©Û’ Ø§Ø¨Ùˆ Ú©ÛŒØ§ Ú©Ø§Ù… Ú©Ø±ØªÛ’ ÛÛŒÚºØŸ</span>â€œ <span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ù…ÛŒÚº Ù†Û’ ØªØ¬Ø³Ø³ Ø³Û’ Ù¾ÙˆÚ†Ú¾Ø§Û”</span><br>â€<span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ø¬ÛŒ</span>â€¤â€¤â€¤â€¤â€¤<span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ø¬ÛŒ</span>â€¤â€¤â€¤â€¤â€¤<span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">ÙˆÛ</span>â€¤â€¤â€¤â€¤â€¤ <span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ø§Ù„Ù„Û Ú©Û’ Ø¯ÙˆØ³Øª ÛÛŒÚºÛ”</span>â€œ <span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ø§Ø³ Ú©Ø§ Ø¬ÙˆØ§Ø¨ Ø³Ù† Ú©Ø± Ú©Ù„Ø§Ø³ Ù…ÛŒÚº Ù‚ÛÙ‚ÛÛ’ Ú¯ÙˆÙ†Ø¬Ù†Û’ Ù„Ú¯Û’ØŒ Ù…Ú¯Ø± Ù…ÛŒØ±ÛŒ Ø³Ù†Ø¬ÛŒØ¯Ú¯ÛŒ Ø¯ÛŒÚ©Ú¾ Ú©Ø± Ø®Ø§Ù…ÙˆØ´ ÛÙˆ Ú¯Ø¦Û’Û”</span><br>â€<span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ø§Ù„Ù„Û Ú©Û’ Ø¯ÙˆØ³Øª</span>â€¤â€¤â€¤â€¤â€¤<span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">ÙˆÛ Ú©ÛŒØ³Û’</span>! <span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ú©ÛŒØ§ Ø¢Ù¾ Ø§Ø³ Ú©ÛŒ ÙˆØ¶Ø§Ø­Øª Ú©Ø±ÛŒÚº Ú¯Û’ØŸ</span>â€œ<br><span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ø¬ÛŒ ÙˆÛ Ù…Ø­Ù†Øª Ù…Ø²Ø¯ÙˆØ±ÛŒ Ú©Ø±ØªÛ’ ÛÛŒÚºÛ”ÛÙ…Ø§Ø±Û’ Ù†Ø¨ÛŒ Ú©Ø±ÛŒÙ… ØµÙ„ÛŒ Ø§Ù„Ù„Û Ø¹Ù„ÛŒÛ Ùˆ Ø¢Ù„Û ÙˆØ³Ù„Ù… Ú©Ø§ Ø§Ø±Ø´Ø§Ø¯ ÛÛ’ Ú©Û ÛØ§ØªÚ¾ Ø³Û’ Ú©Ù…Ø§Ù†Û’ ÙˆØ§Ù„Ø§ Ø§Ù„Ù„Û Ú©Ø§ Ø¯ÙˆØ³Øª ÛÛ’ ØªÙˆ Ù…ÛŒØ±Û’ Ø§Ø¨Ùˆ Ø¨Ú¾ÛŒ Ø§Ù„Ù„Û Ú©Û’ Ø¯ÙˆØ³Øª ÛÙˆØ¦Û’ØŒ Ú©ÛŒÙˆÙ†Ú©Û ÙˆÛ Ø§Ù¾Ù†Û’ ÛØ§ØªÚ¾ Ø³Û’ Ú©Ù…Ø§ØªÛ’ ÛÛŒÚºÛ”</span><div class="clear mt2"></div><span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">ÙˆÛ Ø³Ø§Ø±Ø§ Ø¯Ù† Ù…Ø­Ù†Øª Ù…Ø²Ø¯ÙˆØ±ÛŒ Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ ØªØ§Ú©Û Ù…ÛŒØ±ÛŒ ÙÛŒØ³ Ø§Ø¯Ø§ Ú©Ø± Ø³Ú©ÛŒÚº Ø§ÙˆØ± Ù…Ø¬Ú¾Û’ Ø¨ÛØªØ± Ø³Û’ Ø¨ÛØªØ± ØªØ¹Ù„ÛŒÙ… Ø¯Ù„ÙˆØ§ Ø³Ú©ÛŒÚºÛ”ÙˆÛ Ú©ÛØªÛ’ ÛÛŒÚº ØªÙ… Ø®ÙˆØ¨ Ù…Ø­Ù†Øª Ú©Ø±Ùˆ Ø§ÙˆØ± Ø¨Ú‘Û’ Ø¢Ø¯Ù…ÛŒ Ø¨Ù†Ù†Ø§Û”Ø§Ø³ Ú©Ø§ Ø¬ÙˆØ§Ø¨ Ø³Ù† Ú©Ø± Ù…ÛŒÚº Ø­ÛŒØ±Ø§Ù† Ø±Û Ú¯ÛŒØ§Û”Ø§Ø³ Ù‚Ø¯Ø± Ù¾Ø®ØªÛ ÛŒÙ‚ÛŒÙ† Ú©Ø§ Ø¨Ú†Û Ø¯ÛŒÚ©Ú¾ Ú©Ø± Ø¯Ù„ Ú©Ùˆ Ø³Ú©ÙˆÙ† Ù…Ù„Ø§Û”Ù…ÛŒÚº Ù†Û’ Ø§Ø³Û’ Ø´Ø§Ø¨Ø§Ø´ Ø¯ÛŒ Ø§ÙˆØ± Ù¾ÙˆØ±ÛŒ Ú©Ù„Ø§Ø³ Ø³Û’ Ù…Ø®Ø§Ø·Ø¨ ÛÙˆØ§</span>:â€<span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">ÙˆØ§Ù‚Ø¹ÛŒ ÛØ§ØªÚ¾ Ø³Û’ Ú©Ù…Ø§Ù†Û’ ÙˆØ§Ù„Ø§ Ø§Ù„Ù„Û Ú©Ø§ Ø¯ÙˆØ³Øª ÛÙˆØªØ§ ÛÛ’Û”Ù…Ø­Ù†Øª Ú©ÛŒ Ø¹Ø¸Ù…Øª Ø§ÙˆØ± Ø¨Ø±Ú©Øª Ø³Û’ Ú©ÙˆÙ† ÙˆØ§Ù‚Ù Ù†ÛÛŒÚº ÛÛ’Û”Ø¢Ù¾ Ú©Û’ Ø§Ø¨Ùˆ Ú©Ø§ Ù…Ù‚Ø§Ù… Ø¨ÛØª Ø¨Ù„Ù†Ø¯ ÛÛ’Û”Ø¢Ù¾ Ú©Ùˆ Ø§Ø³ Ù¾Ø± ÙØ®Ø± ÛÙˆÙ†Ø§ Ú†Ø§ÛÛŒÛ’Û”Ú©Ø¦ÛŒ Ø§Ù†Ø¨ÛŒØ§Ø¦Û’ Ú©Ø±Ø§Ù… Ø¨Ú¾ÛŒ Ø§Ù¾Ù†Û’ ÛØ§ØªÚ¾ Ú©ÛŒ Ú©Ù…Ø§Ø¦ÛŒ Ú©Ú¾Ø§ÛŒØ§ Ú©Ø±ØªÛ’ ØªÚ¾Û’Û”</span>â€œ <span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ù…ÛŒÚº Ù†Û’ Ù…Ø­Ø³ÙˆØ³ Ú©ÛŒØ§ Ú©Û Ù…ÛŒØ±ÛŒ Ø¨Ø§Øª Ú©Ø§ Ù¾ÙˆØ±ÛŒ Ú©Ù„Ø§Ø³ Ù¾Ø± Ø§Ø«Ø± ÛÙˆ Ø±ÛØ§ ÛÛ’ Ø§ÙˆØ± Ù…Ø¬Ú¾Û’ Ø®ÙˆØ´ÛŒ ØªÚ¾ÛŒ Ú©Û Ù…ÛŒÚº Ù¾ÙˆØ±ÛŒ Ú©Ù„Ø§Ø³ Ú©Ùˆ Ù…Ø­Ù†Øª Ú©ÛŒ Ø¹Ø¸Ù…Øª Ø³Ù…Ø¬Ú¾Ø§Ù†Û’ Ù…ÛŒÚº Ú©Ø§Ù…ÛŒØ§Ø¨ ÛÙˆØ§Û”</span></div>
                </div>
            </div>
        </div>
    </div>
</div>
~~~


---

## 4

~~~prompt
Almost perfect. here is the code for improving, Give me the complete final version with correcte data cleaning for non urdu characters
```python
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import re
import time
import random
import os

# Configuration
BASE_URL = "https://www.urdupoint.com"
LIST_URL_TEMPLATE = "https://www.urdupoint.com/kids/category/moral-stories-page{}.html"
OUTPUT_DIR = "urdu_stories_dataset"
LINKS_FILE = "all_story_links.txt"

# Requested Tags
TAG_EOS = " <EOS>" 
TAG_EOP = " <EOP>" 
TAG_EOD = " <EOD>" 

# Urdu Sentence Punctuation Regex - Handles typical Urdu stops
URDU_PUNCT_REGEX = r'([Û”ØŸ!])'

if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

def process_urdu_text(text):
    if not text: return ""
    # Split text by punctuation while keeping the punctuation mark
    parts = re.split(URDU_PUNCT_REGEX, text)
    sentences = []
    
    # Reassemble sentences with the punctuation and add EOS tag
    for i in range(0, len(parts) - 1, 2):
        sentence = parts[i].strip() + parts[i+1]
        if sentence:
            sentences.append(sentence + TAG_EOS)
            
    # Handle any trailing text without punctuation
    if len(parts) % 2 != 0 and parts[-1].strip():
        sentences.append(parts[-1].strip() + TAG_EOS)
        
    return " ".join(sentences)

def get_driver():
    options = uc.ChromeOptions()
    options.add_argument("--no-sandbox") 
    options.add_argument("--disable-dev-shm-usage")
    options.page_load_strategy=("eager")
    driver = uc.Chrome(options=options)
    return driver

def collect_story_links(driver, start_page, end_page):
    """First collect all story links from all pages"""
    all_links = []
    
    for page_num in range(start_page, end_page + 1):
        print(f"\n--- Collecting links from Story List Page {page_num} ---")
        driver.get(LIST_URL_TEMPLATE.format(page_num))
        time.sleep(random.uniform(2, 3))
        
        # Wait for the links to load
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, 'a.sharp_box'))
        )
        
        # Find story links using Selenium
        story_links = driver.find_elements(By.CSS_SELECTOR, 'a.sharp_box')
        
        for link in story_links:
            try:
                href = link.get_attribute('href')
                if href:
                    full_url = href if href.startswith('http') else BASE_URL + href
                    all_links.append(full_url)
                    print(f"   Collected: {full_url}")
            except:
                continue  # Skip if we can't get the href
        
        # Save links as we go
        with open(LINKS_FILE, "a", encoding="utf-8") as links_log:
            for url in all_links[-len(story_links):]:  # Just save the newly collected ones
                links_log.write(url + "\n")
    
    return all_links

def scrape_story(driver, url):
    """Scrape a single story using BeautifulSoup on page source"""
    print(f"   Scraping: {url}")
    driver.get(url)
    time.sleep(random.uniform(3, 5))
    
    # Get page source and parse with BeautifulSoup
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    
    # --- 1. Get the Title ---
    title_tag = soup.find('h1', class_='phead')
    title = title_tag.get_text(strip=True) if title_tag else "Untitled Story"
    print(f"   Title: {title}")
    
    # --- 2. Locate the main story container ---
    # Try multiple possible containers (based on observed structure)
    story_container = None
    possible_containers = [
        ('div', {'class': 'txt_detail'}),
        ('div', {'style': 'text-align: right;'}),
        ('div', {'class': 'article_content'})
    ]
    for tag, attrs in possible_containers:
        story_container = soup.find(tag, attrs=attrs)
        if story_container:
            break
    
    if not story_container:
        print("   No story container found")
        return None
    
    # --- 3. Extract paragraphs from the container ---
    # Convert the container to string and split by <br> tags to get paragraphs
    container_html = str(story_container)
    
    # Replace <br>, <br/>, </p> with a special marker (e.g., "||PARAGRAPH||")
    # Also handle <div class="clear"> which sometimes separates paragraphs
    for tag in ['<br>', '<br/>', '</p>', '<div class="clear']:
        container_html = container_html.replace(tag, '||PARAGRAPH||')
    
    # Now parse the modified HTML to get text
    temp_soup = BeautifulSoup(container_html, 'html.parser')
    raw_text = temp_soup.get_text()
    
    # Split by the marker to get paragraphs
    raw_paragraphs = [p.strip() for p in raw_text.split('||PARAGRAPH||') if p.strip()]
    
    # --- 4. Process each paragraph with sentence tags and add <EOP> ---
    processed_paragraphs = []
    for i, para in enumerate(raw_paragraphs):
        if para:
            # Process sentences (add <EOS>)
            p_text = process_urdu_text(para) + TAG_EOP
            processed_paragraphs.append(p_text)
            
            # Debug first paragraph
            if i == 0:
                print(f"   [DEBUG] 1st Paragraph Preview: {p_text[:100]}...")
    
    # --- 5. Save to file ---
    if processed_paragraphs:
        safe_title = "".join(x for x in title[:50] if x.isalnum() or x==' ').strip()
        if not safe_title:
            safe_title = f"story_{int(time.time())}"
        
        file_path = os.path.join(OUTPUT_DIR, f"{safe_title}.txt")
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(f"TITLE: {title}\n\n")
            f.write("\n".join(processed_paragraphs))
            f.write(TAG_EOD)
        print(f"      Saved: {safe_title}.txt")
        return file_path
    else:
        print("   No content extracted")
        return None
    
def scrape_urdu_point(start_page, end_page):
    driver = get_driver()
    try:
        # Step 1: Collect all story links first
        print("=== STEP 1: Collecting all story links ===")
        story_links = collect_story_links(driver, start_page, end_page)
        print(f"\nTotal links collected: {len(story_links)}")
        
        # Step 2: Scrape each story
        print("\n=== STEP 2: Scraping individual stories ===")
        for i, url in enumerate(story_links, 1):
            print(f"\n--- Story {i}/{len(story_links)} ---")
            try:
                scrape_story(driver, url)
                time.sleep(random.uniform(1, 2))  # Be polite between requests
            except Exception as e:
                print(f"   Error scraping {url}: {str(e)}")
                continue
        
        print(f"\n=== Scraping Complete! ===")
        print(f"Links saved to: {LINKS_FILE}")
        print(f"Stories saved to: {OUTPUT_DIR}/")
        
    finally:
        driver.quit()

if __name__ == "__main__":
    scrape_urdu_point(1, 1)
```

Just need to correct the post cleaning of the data as advertisements get in the way along with weird mb15, hidden content like:
<p class="hide_desk ac urdu rtl fs18 lh36" style="color: rgb(102, 102, 102); --darkreader-inline-color: var(--darkreader-text-666666, #fcf0e0);" data-darkreader-inline-color="">(<span class="ar-huruf nastaleeq3" style="font-size:1.25em; line-height:1.8em;">Ø¬Ø§Ø±ÛŒ ÛÛ’</span>)</p>
```Output
TITLE: Allah Ka Dost - Article No. 2923

mb15"> <EOS> <EOP>
mb15"> 
Ù…Ø­Ù…Ø¯ Ø´Ø§ÛØ¯ Ø­ÙÛŒØ¸ <EOS> <EOP>
Ù…ÛŒÚº Ø§ÛŒÚ© Ø§Ø³ØªØ§Ø¯ ÛÙˆÚº Ø§ÙˆØ± Ù…ÛŒØ±Ø§ Ù…Ø¶Ù…ÙˆÙ† Ø§Ø³Ù„Ø§Ù…ÛŒØ§Øª ÛÛ’Û” <EOS> Ù†Ø¦Û’ Ø§Ø³Ú©ÙˆÙ„ Ù…ÛŒÚº Ø¢Ø¬ Ù…ÛŒØ±Ø§ Ù¾ÛÙ„Ø§ Ø¯Ù† ØªÚ¾Ø§Û” <EOS> Ø§Ø³ÛŒ ÙˆØ¬Û Ø³Û’ Ø®ÙˆØ´ÛŒ Ø¨Ú¾ÛŒ ØªÚ¾ÛŒ Ø§ÙˆØ± ÚˆØ± Ø¨Ú¾ÛŒÛ” <EOS> Ø§Ø³ÛŒ Ø®ÙˆØ´ÛŒ Ù…ÛŒÚºØŒ Ù…ÛŒÚº Ù†Û’ Ù†Ø§Ø´ØªÛ Ø¨Ú¾ÛŒ Ø¨Ø±Ø§Ø¦Û’ Ù†Ø§Ù… Ú©ÛŒØ§ Ø§ÙˆØ± ÙˆÙ‚Øª Ø³Û’ Ú©Ú†Ú¾ Ø¯ÛŒØ± Ù¾ÛÙ„Û’ ÛÛŒ Ø§Ø³Ú©ÙˆÙ„ Ù¾ÛÙ†Ú† Ú¯ÛŒØ§Û” <EOS> Ù¾Ø±Ù†Ø³Ù¾Ù„ ØµØ§Ø­Ø¨ Ø³Û’ Ù…Ù„Ù†Û’ Ú©Û’ Ø¨Ø¹Ø¯ Ù…Ø¬Ú¾Û’ Ø§ÛŒÚ© Ú©Ù„Ø§Ø³ Ù…ÛŒÚº Ø¨Ú¾ÛŒØ¬ Ø¯ÛŒØ§ Ú¯ÛŒØ§Û” <EOS> <EOP>
mt2">Ú©Ù„Ø§Ø³ Ø±ÙˆÙ… Ù…ÛŒÚº Ø®ÙˆØ¨ Ø´ÙˆØ± ÛÙˆ Ø±ÛØ§ ØªÚ¾Ø§Û” <EOS> ØªÙ…Ø§Ù… Ø¨Ú†Û’ Ø§Ù¾Ù†ÛŒ Ø¹Ø§Ø¯Øª Ùˆ ÙØ·Ø±Øª Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ Ø²ÙˆØ± Ø²ÙˆØ± Ø³Û’ Ø¨Ø§ØªÛŒÚº Ú©Ø± Ø±ÛÛ’ ØªÚ¾Û’Û” <EOS> Ù…ÛŒÚº Ù†Û’ Ú©Ù…Ø±Û’ Ù…ÛŒÚº Ù‚Ø¯Ù… Ø±Ú©Ú¾Ø§ ØªÙˆ Ø³Ø¨ Ú©Ùˆ Ø³Ø§Ù†Ù¾ Ø³ÙˆÙ†Ú¯Ú¾ Ú¯ÛŒØ§Û” <EOS> Ø³Ø¨ Ú©Û’ Ø³Ø¨ Ø®Ø§Ù…ÙˆØ´ÛŒ Ø³Û’ Ø³ÛŒØ¯Ú¾Û’ Ø¨ÛŒÙ¹Ú¾ Ú¯Ø¦Û’Û” <EOS> Ù¾Ú¾Ø± Ø§Ú†Ø§Ù†Ú© Ú©Ù„Ø§Ø³ Ú©ÛŒ Ø¯Ø§Ø¦ÛŒÚº Ø¬Ø§Ù†Ø¨ Ø³Û’ â€Ú©Ù„Ø§Ø³ Ø§Ø³Ù¹ÛŒÙ†Úˆâ€œ Ú©ÛŒ Ø¢ÙˆØ§Ø² Ú¯ÙˆÙ†Ø¬ÛŒÛ” <EOS> ÛŒÛ ÛŒÙ‚ÛŒÙ†Ø§ Ú©Ù„Ø§Ø³ Ù…Ø§Ù†ÛŒÙ¹Ø± ØªÚ¾Ø§Û” <EOS> Ø§Ø³ Ú©ÛŒ Ø¢ÙˆØ§Ø² Ø³Ù† Ú©Ø± Ø³Ø¨ Ø¨Ú†Û’ Ø¨Ø§ Ø§Ø¯Ø¨ Ú©Ú¾Ú‘Û’ ÛÙˆ Ú¯Ø¦Û’Û” <EOS> <EOP>
â€Ø³ÙÙ¹ ÚˆØ§Ø¤Ù† Ù¾Ù„ÛŒØ²Û” <EOS> <EOP>
(Ø¬Ø§Ø±ÛŒ ÛÛ’) <EOS> <EOP>
">


LIVEAn error occurred. Please try again laterTap to unmuteLearn moreAdvertisement <EOS> <EOP>
">
â€œ Ù…ÛŒÚº Ù†Û’ ÛØ§ØªÚ¾ Ø³Û’ Ø§Ø´Ø§Ø±Û Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ú©ÛØ§Û” <EOS> <EOP>
mt2">Ø³Ø¨ Ø¨ÛŒÙ¹Ú¾ Ú¯Ø¦Û’Û” <EOS> <EOP>
â€Ø´Ø§ÛŒØ¯ Ø¢Ù¾ Ø¬Ø§Ù†ØªÛ’ ÛÙˆÚº Ú©Û Ø§Ø¨ØŒ Ù…ÛŒÚº Ø¢Ù¾ Ú©Ùˆ Ø§Ø³Ù„Ø§Ù…ÛŒØ§Øª Ù¾Ú‘Ú¾Ø§ÛŒØ§ Ú©Ø±ÙˆÚº Ú¯Ø§Û” <EOS> â€œ <EOS> <EOP>
â€ÛŒØ³ Ø³Ø±! <EOS> â€œ Ú†Ù†Ø¯ Ø¨Ú†ÙˆÚº Ú©ÛŒ Ø¢ÙˆØ§Ø² Ø¢Ø¦ÛŒÛ” <EOS> <EOP>
â€Ø¢Ø¬ Ù¾ÛÙ„ÛŒ Ø¨Ø§Ø± Ø¢Ù¾ Ú©ÛŒ Ú©Ù„Ø§Ø³ Ù„Û’ Ø±ÛØ§ ÛÙˆÚºØŒ Ø§Ø³ÛŒ Ù„Ø¦Û’ Ø¢Ø¬ Ú©Ú†Ú¾ Ù†ÛÛŒÚº Ù¾Ú‘Ú¾Ø§Ø¤Úº Ú¯Ø§Û” <EOS> Ù¾ÛÙ„Û’ Ù…ÛŒÚº Ø§Ù¾Ù†Ø§ ØªØ¹Ø§Ø±Ù Ú©Ø±Ø§Ø¤Úº Ú¯Ø§ØŒ Ù¾Ú¾Ø± Ø§ÛŒÚ© Ø§ÛŒÚ© Ú©Ø± Ú©Û’ Ø¢Ù¾ Ø³Ø¨ Ú©Û’ Ø¨Ø§Ø±Û’ Ù…ÛŒÚº Ø¬Ø§Ù†Ù†Ø§ Ú†Ø§ÛÙˆÚº Ú¯Ø§Û” <EOS> â€œ <EOS> <EOP>
Ø§Ø³ØªØ§Ø¯ Ú©Ø§ Ø´Ø§Ú¯Ø±Ø¯ÙˆÚº Ø³Û’ Ø¨Ú‘Ø§ Ú¯ÛØ±Ø§ ØªØ¹Ù„Ù‚ ÛÙˆØªØ§ ÛÛ’Û” <EOS> Ø§Ø³ØªØ§Ø¯ Ù…Ø¹Ù„Ù… ÛÛ’ Ø§ÙˆØ± Ø§Ø³ Ú©Ø§ Ú©Ø§Ù… Ø¹Ù„Ù… Ùˆ Ø¢Ú¯ÛÛŒ Ø¯ÛŒÙ†Ø§ ÛÛ’Û” <EOS> <EOP>
mt2">ÛŒÛÛŒ Ú©Ø§Ù… Ú¯Ú¾Ø± Ù…ÛŒÚº Ù…Ø§Úº Ø¨Ø§Ù¾ Ø¨Ú¾ÛŒ Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” <EOS> Ø§Ø³ÛŒ Ù„Ø­Ø§Ø¸ Ø³Û’ Ú©ÛØ§ Ø¬Ø§ØªØ§ ÛÛ’ Ú©Û Ø§Ø³ØªØ§Ø¯ Ø¨Ø§Ù¾ Ú©ÛŒ Ø¬Ú¯Û ÛÙˆØªØ§ ÛÛ’ØŒ Ù„ÛŒÚ©Ù† Ù…ÛŒØ±Û’ Ù†Ø²Ø¯ÛŒÚ© Ø§Ø³ØªØ§Ø¯ ÛÛŒ Ø¨ÛØªØ±ÛŒÙ† Ø¯ÙˆØ³Øª ÛÛ’Û” <EOS> Ø´Ø§Ú¯Ø±Ø¯ÙˆÚº Ú©Ùˆ Ø§Ø³ØªØ§Ø¯ Ú©Ø§ Ø§Ø­ØªØ±Ø§Ù… Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ø§Ø³ Ø³Û’ Ø¨Û’ ØªÚ©Ù„Ù Ø¨Ú¾ÛŒ ÛÙˆÙ†Ø§ Ú†Ø§ÛÛŒÛ’ØŒ ØªØ§Ú©Û ÙˆÛ Ø§Ù¾Ù†Û’ Ù…Ø³Ø§Ø¦Ù„ Ù¾Ø± Ø§Ø³ØªØ§Ø¯ Ø³Û’ Ø¨Ø§Øª Ú©Ø± Ø³Ú©ÛŒÚºÛ” <EOS> Ø§Ø³ Ø³Û’ Ù…Ø´ÙˆØ±Û Ú©Ø± Ø³Ú©ÛŒÚº Ø§ÙˆØ± Ø§Ø³ØªØ§Ø¯ Ø§Ù† Ú©ÛŒ Ø±Ø§ÛÙ†Ù…Ø§Ø¦ÛŒ Ú©Ø± Ø³Ú©Û’Û” <EOS> Ú†Ù†Ø¯ Ù„Ù…Ø­Û’ Ú©Ù„Ø§Ø³ Ù…ÛŒÚº Ø®Ø§Ù…ÙˆØ´ÛŒ Ú†Ú¾Ø§Ø¦ÛŒ Ø±ÛÛŒ Ù¾Ú¾Ø± Ú©Ú†Ú¾ Ù…Ù„ÛŒ Ø¬Ù„ÛŒ Ø¢ÙˆØ§Ø²ÛŒÚº Ø§ÙØ¨Ú¾Ø±ÛŒÚºÛ” <EOS> <EOP>
mt2"> <EOS> <EOP>
â€ØªÚ¾ÛŒÙ†Ú© ÛŒÙˆ Ø³Ø±! <EOS> â€œ <EOS> <EOP>
â€Ø§Ø¨ Ù…ÛŒÚº Ø§Ù¾Ù†Ø§ ØªØ¹Ø§Ø±Ù Ú©Ø±Ø§ Ø¯ÙˆÚºÛ” <EOS> Ù…ÛŒØ±Ø§ Ù†Ø§Ù… Ø´ÛØ§Ø¨ Ø­ÛŒØ¯Ø± ÛÛ’Û” <EOS> ØµØ­Ø§ÙØª Ù…ÛŒÚº Ø§ÛŒÙ… Ø§Û’ Ú©ÛŒØ§ ÛÛ’ØŒ Ù…Ú¯Ø± Ø¹Ù…Ù„ÛŒ Ø·ÙˆØ± Ù¾Ø± ØªØ¯Ø±ÛŒØ³ÛŒ Ù…ÛŒØ¯Ø§Ù† Ù…ÛŒÚº Ø¢ Ú¯ÛŒØ§Û” <EOS> Ù¾Ú‘Ú¾Ø§Ù†Ø§ Ù…ÛŒØ±Ø§ Ø´ÙˆÙ‚ ÛÛŒ Ù†ÛÛŒÚºØŒ Ø¨Ù„Ú©Û Ø§Ø³ Ø³Û’ Ù…Ø¬Ú¾Û’ Ø¯Ù„ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† ÛÙˆØªØ§ ÛÛ’Û” <EOS> Ù…ÛŒÚº Ù†Û’ Ø§Ø³Ù„Ø§Ù…ÛŒØ§Øª Ú©Ø§ Ù…Ø¶Ù…ÙˆÙ† Ø§Ø³ Ù„Ø¦Û’ Ù…Ù†ØªØ®Ø¨ Ú©ÛŒØ§ ÛÛ’ Ú©Û Ø¢Ù¾ Ú©Ùˆ Ø­Ù‚ÛŒÙ‚ÛŒ Ø§Ø³Ù„Ø§Ù… Ø³Û’ Ø¢Ú¯Ø§Û Ú©Ø±ÙˆÚº Ø§ÙˆØ± Ø¢Ù¾ Ú©Ùˆ Ù…Ø­Ø¨Ù ÙˆØ·Ù† Ø´ÛØ±ÛŒ Ø¨Ù†Ø§Ø¤ÚºÛ” <EOS> Ø§Ø¨ Ø¢Ù¾ Ù„ÙˆÚ¯ Ø¨Ø§Ø±ÛŒ Ø¨Ø§Ø±ÛŒ Ø§Ù¾Ù†Ø§ ØªØ¹Ø§Ø±Ù Ú©Ø±Ø§ Ø¯ÛŒÚºÛ” <EOS> <EOP>
mt2">â€œ Ù…ÛŒÚº Ù†Û’ Ù¾ÛÙ„ÛŒ Ù‚Ø·Ø§Ø± Ù…ÛŒÚº Ø¯Ø§ÛÙ†ÛŒ Ø¬Ø§Ù†Ø¨ Ø¨ÛŒÙ¹Ú¾Û’ Ù„Ú‘Ú©Û’ Ú©ÛŒ Ø·Ø±Ù Ø§Ø´Ø§Ø±Û Ú©ÛŒØ§Û” <EOS> <EOP>
â€Ø³Ø±! <EOS> Ù…ÛŒØ±Ø§ Ù†Ø§Ù… Ø¹Ù…Ø± Ø§Ø­Ù…Ø¯ ÛÛ’Û” <EOS> Ù…ÛŒØ±Û’ Ø§Ø¨Ùˆ Ú©Ø§ Ù†Ø§Ù… Ø­ÛŒØ§Øª Ø§Ø­Ù…Ø¯ ÛÛ’Û” <EOS> ÙˆÛ Ø§ÛŒÚ© Ø¨ÛŒÙ†Ú© Ù…Ù†ÛŒØ¬Ø± ÛÛŒÚºÛ” <EOS> â€œ Ø§Ø³ Ú©Û’ ÙˆØ§Ù„Ø¯ Ú©Ø§ Ø³Ù† Ú©Ø± Ø³Ø§Ø±ÛŒ Ú©Ù„Ø§Ø³ Ù¾Ø± Ø±Ø¹Ø¨ Ø·Ø§Ø±ÛŒ ÛÙˆ Ú¯ÛŒØ§Û” <EOS> Ø§ØªÙ†Û’ Ù…ÛŒÚº Ø¯ÙˆØ³Ø±Ø§ Ù„Ú‘Ú©Ø§ Ú©Ú¾Ú‘Ø§ ÛÙˆ Ú¯ÛŒØ§Û” <EOS> <EOP>
â€Ø³Ø±! <EOS> Ù…ÛŒØ±Ø§ Ù†Ø§Ù… Ø­Ù…Ø²Û ÛÛ’Û” <EOS> Ù…ÛŒØ±Û’ Ø§Ø¨Ùˆ Ø§ÛŒÚ© Ù¾Ø±Ø§Ø¦ÛŒÙˆÛŒÙ¹ ÙØ±Ù… Ù…ÛŒÚº Ø¬Ù†Ø±Ù„ Ù…Ù†ÛŒØ¬Ø± ÛÛŒÚºÛ” <EOS> â€œ <EOS> <EOP>
â€Ø§ÙˆÛ.... ÛŒÛ Ø¨Ú¾ÛŒ Ù…Ù†ÛŒØ¬Ø±â€¤â€¤â€¤â€¤â€œ Ù…ÛŒØ±Û’ Ù…Ù†Û Ø³Û’ Ù†Ú©Ù„Ø§Û” <EOS> <EOP>
mt2">Ø§Ø³ Ú©Û’ Ø¨Ø¹Ø¯ ØªÛŒØ³Ø±Ø§ Ù„Ú‘Ú©Ø§ Ú©Ú¾Ú‘Ø§ ÛÙˆØ§Û” <EOS> ÙˆÛ Ø¨Ú¾ÛŒ Ù¾ÛÙ„Û’ Ø¯ÙˆÙ†ÙˆÚº Ø³Û’ Ú©Ù… Ù†Ø¸Ø± Ù†ÛÛŒÚº Ø¢ Ø±ÛØ§ ØªÚ¾Ø§Û” <EOS> <EOP>
â€Ù…Ø¬Ú¾Û’ Ø°ÛŒØ´Ø§Ù† Ú©ÛØªÛ’ ÛÛŒÚºÛ” <EOS> Ù…ÛŒØ±Û’ Ø§Ø¨Ùˆ Ø§ÛŒÚ© ØªØ§Ø¬Ø± ÛÛŒÚº Ø§Ù† Ú©Ø§ Ú©Ù¾Ú‘Û’ Ú©Ø§ Ú©Ø§Ø±ÙˆØ¨Ø§Ø± ÛÛ’Û” <EOS> â€œ <EOS> <EOP>
Ø§Ù† Ø³Ø¨ Ú©Û’ ØªØ¹Ø§Ø±Ù Ù…ÛŒÚº Ø­ÛŒØ±Ø§Ù† Ú©Ù† Ø¨Ø§Øª Ø§Ù† Ú©Ø§ Ø®Ø§Ù†Ø¯Ø§Ù†ÛŒ Ù¾Ø³ Ù…Ù†Ø¸Ø± ØªÚ¾Ø§ØŒ Ø¬Ø³Û’ ÙˆÛ ÙØ®Ø±ÛŒÛ Ø§Ù†Ø¯Ø§Ø² Ù…ÛŒÚº Ø¨ÛŒØ§Ù† Ú©Ø± Ø±ÛÛ’ ØªÚ¾Û’Û” <EOS> ÛŒÛ Ø¨Ø§Øª Ù…Ø¬Ú¾Û’ Ø§Ú†Ú¾ÛŒ Ù†Û Ù„Ú¯ÛŒÛ” <EOS> Ø¢Ú¯Û’ Ø¨Ú¾ÛŒ ØªØ¹Ø§Ø±Ù ÛÙˆØ§ ØªÙˆ ØªÙ…Ø§Ù… Ù„Ú‘Ú©Û’ Ø§Ø¹Ù„ÛŒÙ° Ø§ÙˆØ± Ú©Ú¾Ø§ØªÛ’ Ù¾ÛŒØªÛ’ Ú¯Ú¾Ø±Ø§Ù†ÙˆÚº Ú©Û’ Ú†Ø´Ù… Ùˆ Ú†Ø±Ø§Øº Ø«Ø§Ø¨Øª ÛÙˆØ¦Û’ØŒ Ú©ÛŒÙˆÙ†Ú©Û ÛŒÛ Ø§ÛŒÚ© Ù…ÛÙ†Ú¯Ø§ Ø§ÙˆØ± Ù…Ø¹ÛŒØ§Ø±ÛŒ Ø§Ø³Ú©ÙˆÙ„ ØªÚ¾Ø§Û” <EOS> <EOP>
mt2">ØºØ±ÛŒØ¨ Ù„ÙˆÚ¯ÙˆÚº Ú©Û’ Ø¨Ú†Û’ ØªÙˆ Ø§Ø³ Ú©Ø§ ØµØ±Ù Ø³ÙˆÚ† Ø³Ú©ØªÛ’ ØªÚ¾Û’Û” <EOS> Ø§Ø¨Ú¾ÛŒ Ø§Ù†Ú¾ÛŒ Ø®ÛŒØ§Ù„ÙˆÚº Ù…ÛŒÚº Ù…Ú¯Ù† ØªÚ¾Ø§ Ú©Û Ø§ÛŒÚ© Ù„Ú‘Ú©Ø§ Ø¬Ùˆ Ù„Ø§Ø¦Ù† Ú©Û’ Ø¢Ø®Ø±ÛŒ ÚˆÛŒØ³Ú© Ù¾Ø± Ø¨ÛŒÙ¹Ú¾Ø§ ØªÚ¾Ø§ØŒ Ø§ÙÙ¹Ú¾ Ú©Ú¾Ú‘Ø§ ÛÙˆØ§Û” <EOS> Ø§Ø³ Ú©Ø§ Ú†ÛØ±Û Ø§Ø¹ØªÙ…Ø§Ø¯ Ø³Û’ Ø®Ø§Ù„ÛŒ Ù†Ø¸Ø± Ø¢ Ø±ÛØ§ ØªÚ¾Ø§Û” <EOS> Ù…ÛŒÚº Ù†Û’ Ø§Ø³ Ø³Û’ ØªØ¹Ø§Ø±Ù Ú©Û’ Ù„Ø¦Û’ Ú©ÛØ§ ØªÙˆ ÙˆÛ Ù‚Ø¯Ø±Û’ ÛÚ†Ú©Ú†Ø§ØªÛ’ ÛÙˆØ¦Û’ Ø¨ÙˆÙ„Ø§:â€Ø³Ø±! <EOS> Ù…ÛŒØ±Ø§ Ù†Ø§Ù… Ù…Ø­Ù…Ø¯ Ø¹Ù„ÛŒ ÛÛ’Û” <EOS> Ù…ÛŒØ±Ø§ ØªØ¹Ù„Ù‚ Ø§ÛŒÚ© Ø¹Ø§Ù… Ø³Û’ Ú¯Ú¾Ø±Ø§Ù†Û’ Ø³Û’ ÛÛ’Û” <EOS> â€œ Ú©Ù„Ø§Ø³ Ú©Û’ ØªÙ…Ø§Ù… Ù„Ú‘Ú©Û’ Ø§Ø³ Ú©ÛŒ Ø·Ø±Ù Ø¯ÛŒÚ©Ú¾Ù†Û’ Ù„Ú¯Û’ ØªÙˆ ÙˆÛ Ø´Ø±Ù…Ù†Ø¯Û Ø³Ø§ ÛÙˆ Ú¯ÛŒØ§Û” <EOS> <EOP>
mt2"> <EOS> <EOP>
â€Ø¢Ù¾ Ú©Û’ Ø§Ø¨Ùˆ Ú©ÛŒØ§ Ú©Ø§Ù… Ú©Ø±ØªÛ’ ÛÛŒÚºØŸ <EOS> â€œ Ù…ÛŒÚº Ù†Û’ ØªØ¬Ø³Ø³ Ø³Û’ Ù¾ÙˆÚ†Ú¾Ø§Û” <EOS> <EOP>
â€Ø¬ÛŒâ€¤â€¤â€¤â€¤â€¤Ø¬ÛŒâ€¤â€¤â€¤â€¤â€¤ÙˆÛâ€¤â€¤â€¤â€¤â€¤ Ø§Ù„Ù„Û Ú©Û’ Ø¯ÙˆØ³Øª ÛÛŒÚºÛ” <EOS> â€œ Ø§Ø³ Ú©Ø§ Ø¬ÙˆØ§Ø¨ Ø³Ù† Ú©Ø± Ú©Ù„Ø§Ø³ Ù…ÛŒÚº Ù‚ÛÙ‚ÛÛ’ Ú¯ÙˆÙ†Ø¬Ù†Û’ Ù„Ú¯Û’ØŒ Ù…Ú¯Ø± Ù…ÛŒØ±ÛŒ Ø³Ù†Ø¬ÛŒØ¯Ú¯ÛŒ Ø¯ÛŒÚ©Ú¾ Ú©Ø± Ø®Ø§Ù…ÙˆØ´ ÛÙˆ Ú¯Ø¦Û’Û” <EOS> <EOP>
â€Ø§Ù„Ù„Û Ú©Û’ Ø¯ÙˆØ³Øªâ€¤â€¤â€¤â€¤â€¤ÙˆÛ Ú©ÛŒØ³Û’! <EOS> Ú©ÛŒØ§ Ø¢Ù¾ Ø§Ø³ Ú©ÛŒ ÙˆØ¶Ø§Ø­Øª Ú©Ø±ÛŒÚº Ú¯Û’ØŸ <EOS> â€œ <EOS> <EOP>
Ø¬ÛŒ ÙˆÛ Ù…Ø­Ù†Øª Ù…Ø²Ø¯ÙˆØ±ÛŒ Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” <EOS> ÛÙ…Ø§Ø±Û’ Ù†Ø¨ÛŒ Ú©Ø±ÛŒÙ… ØµÙ„ÛŒ Ø§Ù„Ù„Û Ø¹Ù„ÛŒÛ Ùˆ Ø¢Ù„Û ÙˆØ³Ù„Ù… Ú©Ø§ Ø§Ø±Ø´Ø§Ø¯ ÛÛ’ Ú©Û ÛØ§ØªÚ¾ Ø³Û’ Ú©Ù…Ø§Ù†Û’ ÙˆØ§Ù„Ø§ Ø§Ù„Ù„Û Ú©Ø§ Ø¯ÙˆØ³Øª ÛÛ’ ØªÙˆ Ù…ÛŒØ±Û’ Ø§Ø¨Ùˆ Ø¨Ú¾ÛŒ Ø§Ù„Ù„Û Ú©Û’ Ø¯ÙˆØ³Øª ÛÙˆØ¦Û’ØŒ Ú©ÛŒÙˆÙ†Ú©Û ÙˆÛ Ø§Ù¾Ù†Û’ ÛØ§ØªÚ¾ Ø³Û’ Ú©Ù…Ø§ØªÛ’ ÛÛŒÚºÛ” <EOS> <EOP>
mt2">ÙˆÛ Ø³Ø§Ø±Ø§ Ø¯Ù† Ù…Ø­Ù†Øª Ù…Ø²Ø¯ÙˆØ±ÛŒ Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ ØªØ§Ú©Û Ù…ÛŒØ±ÛŒ ÙÛŒØ³ Ø§Ø¯Ø§ Ú©Ø± Ø³Ú©ÛŒÚº Ø§ÙˆØ± Ù…Ø¬Ú¾Û’ Ø¨ÛØªØ± Ø³Û’ Ø¨ÛØªØ± ØªØ¹Ù„ÛŒÙ… Ø¯Ù„ÙˆØ§ Ø³Ú©ÛŒÚºÛ” <EOS> ÙˆÛ Ú©ÛØªÛ’ ÛÛŒÚº ØªÙ… Ø®ÙˆØ¨ Ù…Ø­Ù†Øª Ú©Ø±Ùˆ Ø§ÙˆØ± Ø¨Ú‘Û’ Ø¢Ø¯Ù…ÛŒ Ø¨Ù†Ù†Ø§Û” <EOS> Ø§Ø³ Ú©Ø§ Ø¬ÙˆØ§Ø¨ Ø³Ù† Ú©Ø± Ù…ÛŒÚº Ø­ÛŒØ±Ø§Ù† Ø±Û Ú¯ÛŒØ§Û” <EOS> Ø§Ø³ Ù‚Ø¯Ø± Ù¾Ø®ØªÛ ÛŒÙ‚ÛŒÙ† Ú©Ø§ Ø¨Ú†Û Ø¯ÛŒÚ©Ú¾ Ú©Ø± Ø¯Ù„ Ú©Ùˆ Ø³Ú©ÙˆÙ† Ù…Ù„Ø§Û” <EOS> Ù…ÛŒÚº Ù†Û’ Ø§Ø³Û’ Ø´Ø§Ø¨Ø§Ø´ Ø¯ÛŒ Ø§ÙˆØ± Ù¾ÙˆØ±ÛŒ Ú©Ù„Ø§Ø³ Ø³Û’ Ù…Ø®Ø§Ø·Ø¨ ÛÙˆØ§:â€ÙˆØ§Ù‚Ø¹ÛŒ ÛØ§ØªÚ¾ Ø³Û’ Ú©Ù…Ø§Ù†Û’ ÙˆØ§Ù„Ø§ Ø§Ù„Ù„Û Ú©Ø§ Ø¯ÙˆØ³Øª ÛÙˆØªØ§ ÛÛ’Û” <EOS> Ù…Ø­Ù†Øª Ú©ÛŒ Ø¹Ø¸Ù…Øª Ø§ÙˆØ± Ø¨Ø±Ú©Øª Ø³Û’ Ú©ÙˆÙ† ÙˆØ§Ù‚Ù Ù†ÛÛŒÚº ÛÛ’Û” <EOS> Ø¢Ù¾ Ú©Û’ Ø§Ø¨Ùˆ Ú©Ø§ Ù…Ù‚Ø§Ù… Ø¨ÛØª Ø¨Ù„Ù†Ø¯ ÛÛ’Û” <EOS> Ø¢Ù¾ Ú©Ùˆ Ø§Ø³ Ù¾Ø± ÙØ®Ø± ÛÙˆÙ†Ø§ Ú†Ø§ÛÛŒÛ’Û” <EOS> Ú©Ø¦ÛŒ Ø§Ù†Ø¨ÛŒØ§Ø¦Û’ Ú©Ø±Ø§Ù… Ø¨Ú¾ÛŒ Ø§Ù¾Ù†Û’ ÛØ§ØªÚ¾ Ú©ÛŒ Ú©Ù…Ø§Ø¦ÛŒ Ú©Ú¾Ø§ÛŒØ§ Ú©Ø±ØªÛ’ ØªÚ¾Û’Û” <EOS> â€œ Ù…ÛŒÚº Ù†Û’ Ù…Ø­Ø³ÙˆØ³ Ú©ÛŒØ§ Ú©Û Ù…ÛŒØ±ÛŒ Ø¨Ø§Øª Ú©Ø§ Ù¾ÙˆØ±ÛŒ Ú©Ù„Ø§Ø³ Ù¾Ø± Ø§Ø«Ø± ÛÙˆ Ø±ÛØ§ ÛÛ’ Ø§ÙˆØ± Ù…Ø¬Ú¾Û’ Ø®ÙˆØ´ÛŒ ØªÚ¾ÛŒ Ú©Û Ù…ÛŒÚº Ù¾ÙˆØ±ÛŒ Ú©Ù„Ø§Ø³ Ú©Ùˆ Ù…Ø­Ù†Øª Ú©ÛŒ Ø¹Ø¸Ù…Øª Ø³Ù…Ø¬Ú¾Ø§Ù†Û’ Ù…ÛŒÚº Ú©Ø§Ù…ÛŒØ§Ø¨ ÛÙˆØ§Û” <EOS> <EOP> <EOD>
```
~~~

---

## 5

~~~prompt
It Works! Now we just need to do final adjustments, since we are training a simple model remove all qoutation marks, don't just add <EOS> replace the Û”ØŸ! 
~~~

---

## 6

~~~prompt
replace .... with " "
give me only that code
~~~

---

## 7

~~~prompt
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import re
import time
import random
import os

# Configuration
BASE_URL = "https://www.urdupoint.com"
LIST_URL_TEMPLATE = "https://www.urdupoint.com/kids/category/moral-stories-page{}.html"
OUTPUT_DIR = "urdu_stories_dataset"
LINKS_FILE = "all_story_links.txt"

# Requested Tags
TAG_EOS = " <EOS>"
TAG_EOP = " <EOP>"
TAG_EOD = " <EOD>"

# Urdu Unicode ranges (for detecting actual Urdu text)
URDU_UNICODE_RANGE = r'[\u0600-\u06FF\u0750-\u077F\uFB50-\uFDFF\uFE70-\uFEFF]'

# Common boilerplate / ad phrases to remove
BOILERPLATE_PHRASES = [
    "Ø¬Ø§Ø±ÛŒ ÛÛ’",
    "LIVEAn error occurred",
    "Advertisement",
    "Please try again later",
    "Tap to unmute",
    "Learn more"
]

if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

def contains_urdu(text):
    """Return True if the text contains at least one Urdu character."""
    return bool(re.search(URDU_UNICODE_RANGE, text))

def is_boilerplate(text):
    """Return True if the text matches any known boilerplate phrase."""
    text_clean = re.sub(r'\s+', '', text)  # remove whitespace for comparison
    for phrase in BOILERPLATE_PHRASES:
        if phrase in text_clean:
            return True
    return False

def clean_paragraph(raw_para):
    """
    Remove HTML artifacts and non-Urdu fragments from a paragraph string.
    Returns cleaned text or None if the paragraph should be discarded.
    """
    # Remove leading HTML-like remnants (e.g., 'mb15">')
    cleaned = re.sub(r'^[^>\s]*>', '', raw_para)
    cleaned = cleaned.strip()

    # Discard if empty or too short (less than 3 characters)
    if len(cleaned) < 3:
        return None

    # Discard if it contains no Urdu characters
    if not contains_urdu(cleaned):
        return None

    # Discard if it's known boilerplate
    if is_boilerplate(cleaned):
        return None

    return cleaned

def process_urdu_text(text):
    """
    Process Urdu text:
      1. Remove all quotation marks.
      2. Replace multiple dots (....) with a space.
      3. Replace Urdu punctuation (Û” ØŸ !) with <EOS> (the punctuation is removed).
      4. Ensure the text ends with <EOS> if it doesn't already.
    Returns a string with sentences separated by <EOS>.
    """
    if not text:
        return ""

    # 1. Remove quotation marks (including curly quotes)
    quote_chars = '"\'"â€â€œâ€˜â€™'
    translator = str.maketrans('', '', quote_chars)
    text = text.translate(translator)

    # 2. Replace multiple dots (ellipsis) with a space
    text = re.sub(r'\â€¤{1,}', ' ', text)
    text = re.sub(r'\.{1,}', ' ', text)

    # 3. Replace Urdu punctuation with <EOS>
    punct_marks = ['Û”', 'ØŸ', '!']
    for punct in punct_marks:
        text = text.replace(punct, f' {TAG_EOS.strip()} ')

    # 4. Clean up extra spaces
    text = re.sub(r'\s+', ' ', text).strip()

    # 5. Ensure the last sentence gets an <EOS> if missing
    if not text.endswith(TAG_EOS):
        text += TAG_EOS

    return text

def get_driver():
    options = uc.ChromeOptions()
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.page_load_strategy = "eager"
    driver = uc.Chrome(options=options)
    return driver

def collect_story_links(driver, start_page, end_page):
    """First collect all story links from all pages."""
    all_links = []

    for page_num in range(start_page, end_page + 1):
        print(f"\n--- Collecting links from Story List Page {page_num} ---")
        driver.get(LIST_URL_TEMPLATE.format(page_num))
        time.sleep(random.uniform(.5, 1))

        # Wait for the links to load
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, 'a.sharp_box'))
        )

        story_links = driver.find_elements(By.CSS_SELECTOR, 'a.sharp_box')

        for link in story_links:
            try:
                href = link.get_attribute('href')
                if href:
                    full_url = href if href.startswith('http') else BASE_URL + href
                    all_links.append(full_url)
                    print(f"   Collected: {full_url}")
            except:
                continue  # Skip if we can't get the href

        # Save links as we go
        with open(LINKS_FILE, "a", encoding="utf-8") as links_log:
            for url in all_links[-len(story_links):]:  # Just save the newly collected ones
                links_log.write(url + "\n")

    return all_links

def scrape_story(driver, url):
    """Scrape a single story using BeautifulSoup on page source."""
    print(f"   Scraping: {url}")
    driver.get(url)
    time.sleep(random.uniform(.5, 1))

    # Get page source and parse with BeautifulSoup
    soup = BeautifulSoup(driver.page_source, 'html.parser')

    # --- 1. Get the Title ---
    title_tag = soup.find('h1', class_='phead')
    title = title_tag.get_text(strip=True) if title_tag else "Untitled Story"
    print(f"   Title: {title}")

    # --- 2. Locate the main story container ---
    story_container = None
    possible_containers = [
        ('div', {'class': 'txt_detail'}),
        ('div', {'style': 'text-align: right;'}),
        ('div', {'class': 'article_content'})
    ]
    for tag, attrs in possible_containers:
        story_container = soup.find(tag, attrs=attrs)
        if story_container:
            break

    if not story_container:
        print("   No story container found")
        return None

    # --- 3. Remove unwanted elements that contain boilerplate or ads ---
    # Remove elements with class 'hide_desk' (often contain "Ø¬Ø§Ø±ÛŒ ÛÛ’")
    for hidden in story_container.find_all(class_='hide_desk'):
        hidden.decompose()
    # Remove script tags and other noise
    for script in story_container.find_all(['script', 'ins', 'iframe']):
        script.decompose()

    # --- 4. Extract paragraphs by splitting on <br> and block elements ---
    # Convert container to string and insert paragraph markers at natural breaks
    container_html = str(story_container)

    # Replace <br> and </p> with a paragraph marker
    for tag in ['<br>', '<br/>', '</p>']:
        container_html = container_html.replace(tag, '||PARAGRAPH||')

    # Also handle <div class="clear"> which often separates paragraphs
    container_html = re.sub(r'<div\s+class="clear[^>]*>', '||PARAGRAPH||', container_html)

    # Now parse the modified HTML to get text
    temp_soup = BeautifulSoup(container_html, 'html.parser')
    raw_text = temp_soup.get_text()

    # Split by the marker to get raw paragraphs
    raw_paragraphs = [p.strip() for p in raw_text.split('||PARAGRAPH||') if p.strip()]

    # --- 5. Clean and filter paragraphs ---
    cleaned_paragraphs = []
    for para in raw_paragraphs:
        cleaned = clean_paragraph(para)
        if cleaned:
            cleaned_paragraphs.append(cleaned)

    # --- 6. Process each cleaned paragraph with sentence tags and add <EOP> ---
    processed_paragraphs = []
    for i, para in enumerate(cleaned_paragraphs):
        if para:
            # process_urdu_text now removes quotes and replaces punctuation with <EOS>
            p_text = process_urdu_text(para) + TAG_EOP
            processed_paragraphs.append(p_text)
            if i == 0:
                print(f"   [DEBUG] 1st Paragraph Preview: {p_text[:100]}...")

    # --- 7. Save to file ---
    if processed_paragraphs:
        title = re.sub(r'\s+', '+_', title)
        safe_title = "".join(x for x in title[:50] if x.isalnum() or x == '_').strip()
        if not safe_title:
            safe_title = f"story_{int(time.time())}"

        file_path = os.path.join(OUTPUT_DIR, f"{safe_title}.txt")
        with open(file_path, "w", encoding="utf-8") as f:
            # f.write(f"TITLE: {title}\n\n")
            f.write("".join(processed_paragraphs[1:]))
            f.write(TAG_EOD)
        print(f"      Saved: {safe_title}.txt")
        return file_path
    else:
        print("   No content extracted")
        return None

def scrape_urdu_point(start_page, end_page):
    driver = get_driver()
    try:
        # Step 1: Collect all story links first
        print("=== STEP 1: Collecting all story links ===")
        story_links = collect_story_links(driver, start_page, end_page)
        print(f"\nTotal links collected: {len(story_links)}")

        # Step 2: Scrape each story
        print("\n=== STEP 2: Scraping individual stories ===")
        for i, url in enumerate(story_links, 1):
            print(f"\n--- Story {i}/{len(story_links)} ---")
            try:
                scrape_story(driver, url)
                time.sleep(random.uniform(.5, 1))  # Be polite between requests
            except Exception as e:
                print(f"   Error scraping {url}: {str(e)}")
                continue

        print(f"\n=== Scraping Complete! ===")
        print(f"Links saved to: {LINKS_FILE}")
        print(f"Stories saved to: {OUTPUT_DIR}/")

    finally:
        driver.quit()
    driver.quit()

if __name__ == "__main__":
    # Start with page 1 only for testing; change as needed
    scrape_urdu_point(1, 1)

Instead of extracting all links and then moving to step 2 is it possible to extract a story link, scrap and save the story and move to the next
~~~
